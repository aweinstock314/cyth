"""
#python -c "import cyth, doctest; print(doctest.testmod(cyth.cyth_benchmarks))"
"""
from __future__ import absolute_import, division, print_function
import utool
import doctest
import ast
import astor
from cyth import cyth_helpers


def get_bench_text_fmt():
    bench_text_fmt_ = r'''
    " Autogenerated by cyth on {timestamp} "
    #!/usr/bin/env python
    from __future__ import absolute_import, division, print_function
    import timeit
    import textwrap
    import warnings
    import utool
    import six
    warnings.simplefilter('ignore', SyntaxWarning)
    warnings.simplefilter('ignore', RuntimeWarning)
    print, print_, printDBG, rrr, profile = utool.inject(__name__, '[{py_modname}.bench]')


    def run_doctest(pyth_call, cyth_call, setup_script):
        setup_globals_py, setup_locals_py = {{}}, {{}}
        setup_globals_cy, setup_locals_cy = {{}}, {{}}
        six.exec_(setup_script, setup_globals_py, setup_locals_py)
        six.exec_(setup_script, setup_globals_cy, setup_locals_cy)
        pyth_result = eval(cyth_call, setup_globals_py, setup_locals_py)
        cyth_result = eval(cyth_call, setup_globals_cy, setup_locals_cy)
        if repr(pyth_result) == repr(cyth_result):
            #print('%r and %r have the same result.' % (pyth_call, cyth_call))
            print('PASS: output is equivalent')
            #%r and %r have the same result.' % (pyth_call, cyth_call))
        else:
            print('<FAILED>')
            print('INCONSISTENCY: %r has different output than %r' % (pyth_call, cyth_call))
            print('___________')
            print('pyth_result')
            print('-----------')
            print(repr(pyth_result))
            print('=========== (end pyth_result)')
            print('___________')
            print('cyth_result')
            print('-----------')
            print(repr(cyth_result))
            print('=========== (end cyth_result)')
            print('</FAILED>')


    {codes}


    def run_all_benchmarks(iterations):
        print('\n\n')
        print('=======================================')
        print('[cyth] Run benchmarks for: {py_modname}')
        with utool.Indenter(' *  '):
            results = []
            {all_benchmarks}
        #sorted_results = sorted(results)
        #sorted_lines = [tup[2] for tup in sorted_results]
        #print('\n'.join(utool.flatten(sorted_lines)))
        return results


    if __name__ == '__main__':
        iterations = utool.get_arg(('--iterations', '-n'), type_=int, default=100)
        run_all_benchmarks(iterations)
    '''
    bench_text_fmt = utool.unindent(bench_text_fmt_).strip('\n')
    return bench_text_fmt


def make_bench_text(benchmark_codes, benchmark_names, py_modname):
    # TODO: let each function individually specify number
    codes = '\n\n\n'.join(benchmark_codes)  # NOQA
    list_ = [utool.quasiquote('results.extend({benchfunc}(iterations))')
             for benchfunc in benchmark_names]
    all_benchmarks = utool.indent('\n'.join(list_), ' ' * 8).strip()  # NOQA
    timestamp = utool.get_timestamp()  # NOQA
    bench_text_fmt = get_bench_text_fmt()
    bench_text = utool.quasiquote(bench_text_fmt)
    return bench_text


def parse_benchmarks(funcname, docstring, py_modname):
    test_tuples_, setup_script_ = make_benchmarks(funcname, docstring, py_modname)
    if len(test_tuples_) == 0:
        test_tuples = '[]'
    else:
        test_tuples = '[\n' + (' ' * 8)  + '\n    '.join(list(map(str, test_tuples_))) + '\n    ]'  # NOQA
    setup_script = utool.indent(setup_script_).strip()  # NOQA
    benchmark_name = utool.quasiquote('run_benchmark_{funcname}')
    #test_tuples, setup_script = make_benchmarks('''{funcname}''', '''{docstring}''')
    # http://en.wikipedia.org/wiki/Relative_change_and_difference
    bench_code_fmt_ = r'''
    def {benchmark_name}(iterations):
        test_tuples = {test_tuples}
        setup_script = textwrap.dedent("""
        {setup_script}
        """)
        time_line = lambda line: timeit.timeit(stmt=line, setup=setup_script, number=iterations)
        time_pair = lambda (x, y): (time_line(x), time_line(y))
        def print_timing_info(tup):
            from math import log
            test_lines = []
            def test_print(str):
                if not utool.QUIET:
                    print(str)
                test_lines.append(str)
            test_print('\n---------------')
            test_print('[bench] timing {benchmark_name} for %d iterations' % (iterations))
            test_print('[bench] tests:')
            test_print('    ' + str(tup))
            (pyth_time, cyth_time) = time_pair(tup)
            test_print("[bench.python] {funcname} time=%f seconds" % (pyth_time))
            test_print("[bench.cython] {funcname} time=%f seconds" % (cyth_time))
            time_delta = cyth_time - pyth_time
            #pcnt_change_wrt_pyth = (time_delta / pyth_time) * 100
            #pcnt_change_wrt_cyth = (time_delta / cyth_time) * 100
            pyth_per_cyth = (pyth_time / cyth_time) * 100
            inv_cyth_per_pyth = 1 / (cyth_time / pyth_time) * 100
            nepers  = log(cyth_time / pyth_time)
            if time_delta < 0:
                test_print('[bench.result] cython was %.1f%% of the speed of python' % (inv_cyth_per_pyth,))
                #test_print('[bench.result] cython was %.1fx faster' % (-pcnt_change_wrt_pyth,))
                test_print('[bench.result] cython was %.1f nepers faster' % (-nepers,))
                test_print('[bench.result] cython was faster by %f seconds' % -time_delta)
            else:
                test_print('[bench.result] cython was %.1f%% of the speed of python' % (pyth_per_cyth,))
                #test_print('[bench.result] cython was %.1fx slower' % (pcnt_change_wrt_pyth,))
                test_print('[bench.result] cython was %.1f nepers slower' % (nepers,))
                test_print('[bench.result] python was faster by %f seconds' % time_delta)
            pyth_call, cyth_call = tup
            run_doctest(pyth_call, cyth_call, setup_script)
            return (pyth_time, cyth_time, test_lines)
        test_results = list(map(print_timing_info, test_tuples))
        # results are lists of (time1, time2, strlist)
        return test_results'''
    bench_code_fmt = utool.unindent(bench_code_fmt_).strip('\n')
    bench_code = utool.quasiquote(bench_code_fmt)
    return (benchmark_name, bench_code)


def replace_funcalls(source, funcname, replacement):
    """
    >>> from cyth_script import *  # NOQA
    >>> replace_funcalls('foo(5)', 'foo', 'bar')
    'bar(5)'
    >>> replace_funcalls('foo(5)', 'bar', 'baz')
    'foo(5)'
    """

    # FIXME: !!!
    # http://docs.cython.org/src/userguide/wrapping_CPlusPlus.html#nested-class-declarations
    # C++ allows nested class declaration. Class declarations can also be nested in Cython:
    # Note that the nested class is declared with a cppclass but without a cdef.
    class FunctioncallReplacer(ast.NodeTransformer):
        def visit_Call(self, node):
            if isinstance(node.func, ast.Name) and node.func.id == funcname:
                node.func.id = replacement
            return node
    generator = astor.codegen.SourceGenerator(' ' * 4)
    generator.visit(FunctioncallReplacer().visit(ast.parse(source)))
    return ''.join(generator.result)
    #return ast.dump(tree)


def parse_doctest_examples(source):
    # parse list of docstrings
    comment_iter = doctest.DocTestParser().parse(source)
    # remove any non-doctests
    example_list = [c for c in comment_iter if isinstance(c, doctest.Example)]
    return example_list


def get_benchline(src, funcname):
    """ Returns the  from a doctest source """
    pt = ast.parse(src)
    assert isinstance(pt, ast.Module), type(pt)
    body = pt.body
    if len(body) != 1:
        return None
    stmt = body[0]
    if not isinstance(stmt, (ast.Expr, ast.Assign)):
        return None
    if isinstance(stmt.value, ast.Call) and isinstance(stmt.value.func, ast.Name):
        if stmt.value.func.id == funcname:
            benchline = cyth_helpers.ast_to_sourcecode(stmt.value)
            return benchline


def make_benchmarks(funcname, docstring, py_modname):
    r"""
    >>> from cyth.cyth_script import *  # NOQA
    >>> funcname = 'replace_funcalls'
    >>> docstring = replace_funcalls.func_doc
    >>> py_modname = 'cyth.cyth_script'
    >>> benchmark_list = list(make_benchmarks(funcname, docstring, py_modname))
    >>> print(benchmark_list)
    [[("replace_funcalls('foo(5)', 'foo', 'bar')", "_replace_funcalls_cyth('foo(5)', 'foo', 'bar')"), ("replace_funcalls('foo(5)', 'bar', 'baz')", "_replace_funcalls_cyth('foo(5)', 'bar', 'baz')")], "from cyth.cyth_script import _replace_funcalls_cyth\nfrom cyth_script import *  # NOQA\nreplace_funcalls('foo(5)', 'foo', 'bar')\nreplace_funcalls('foo(5)', 'bar', 'baz')\n"]

    #>>> output = [((x.source, x.want), y.source, y.want) for x, y in benchmark_list]
    #>>> print(utool.hashstr(repr(output)))
    """
    doctest_examples = parse_doctest_examples(docstring)
    test_lines = []
    cyth_lines = []
    setup_lines = []
    cyth_funcname = cyth_helpers.get_cyth_name(funcname)
    for example in doctest_examples:
        benchline = get_benchline(example.source, funcname)
        if benchline is not None:
            test_lines.append(benchline)
            cyth_lines.append(replace_funcalls(benchline, funcname, cyth_funcname))
            setup_lines.append(example.source)
        else:
            setup_lines.append(example.source)
    test_tuples = list(zip(test_lines, cyth_lines))
    setup_script = utool.unindent(''.join(setup_lines))
    #modname = 'vtool.keypoint'
    setup_script = 'from %s import %s\n' % (py_modname, cyth_funcname,) + setup_script
    return test_tuples, setup_script


def build_runbench_shell_text(cy_bench_list):
    # write script to run all cyth benchmarks
    cmd_list = ['python ' + bench + ' $*'
                for bench in cy_bench_list]
    runbench_text = '\n'.join(['#!/bin/bash'] + cmd_list)
    return runbench_text


def build_runbench_pyth_text(cy_bench_list):
    # write script to run all cyth benchmarks
    runbench_pytext_fmt_ = r'''
    #!/usr/bin/env python
    " Autogenerated by cyth on {timestamp} "
    from __future__ import absolute_import, division, print_function
    import utool
    {bench_import_text}

    SORTBY = utool.get_arg('--sortby', str, 'python')

    if __name__ == '__main__':
        all_results = []
        iterations = utool.get_arg(('--iterations', '-n'), type_=int, default=100)

        # Run the benchmarks
        {bench_runline_text}
        # Sort by chosen field
        sortable_fields = ['python', 'cython']
        sortx = sortable_fields.index(SORTBY)
        sorted_allresults = sorted(all_results, key=lambda tup: tup[sortx])
        sorted_lines = [tup[2] for tup in sorted_allresults]
        # Report sorted results
        print('\n\n')
        print('==================================')
        print('Aggregating all benchmarks results')
        print('==================================')
        print('\n')
        print('sorting by %s' % sortable_fields[sortx])
        print('\n'.join(utool.flatten(sorted_lines)))
    '''
    runbench_pytext_fmt = utool.unindent(runbench_pytext_fmt_).strip('\n')
    from os.path import relpath, splitext
    import os
    def bench_fpath_to_modname(bench):
        bench_upath = utool.unixpath(bench)
        bench_relpath = relpath(bench_upath, os.getcwd())
        bench_relname, _ = splitext(bench_relpath)
        bench_modname = bench_relname.replace('\\', '/').replace('/', '.')
        return bench_modname

    bench_modnames = list(map(bench_fpath_to_modname, cy_bench_list))

    bench_imports = ['import ' + bench_modname for bench_modname in bench_modnames]
    runline_fmt = 'all_results.extend({bench_modname}.run_all_benchmarks(iterations))'
    bench_runlines = [runline_fmt.format(bench_modname=bench_modname)
                      for bench_modname in bench_modnames]
    bench_import_text = '\n'.join(bench_imports)
    bench_runline_text = '\n    '.join(bench_runlines)
    timestamp = utool.get_timestamp()  # NOQA
    runbench_pytext = runbench_pytext_fmt.format(timestamp=timestamp,
                                                 bench_runline_text=bench_runline_text,
                                                 bench_import_text=bench_import_text)
    return runbench_pytext
